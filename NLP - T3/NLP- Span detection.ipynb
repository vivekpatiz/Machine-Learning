{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f81e9de-4b5e-4826-889a-4a78775d7c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Error loading stopwards: Package 'stopwards' not found in\n",
      "[nltk_data]     index\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwards')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.data.path.append(r'C:\\Users\\USER\\AppData\\Roaming\\nltk_data')\n",
    "import pandas as pd\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4e4ff50-aace-41a3-84a9-9a314e081c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>life Goes until jurong point, crazy.. Availabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2\n",
       "0   ham  life Goes until jurong point, crazy.. Availabl...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"spam_sms.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9933843-f153-470a-86e1-0305d65d4a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['label','content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd12b538-d770-4d1f-8b23-17c293d43239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>life Goes until jurong point, crazy.. Availabl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ì_ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                            content\n",
       "0      ham  life Goes until jurong point, crazy.. Availabl...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham              Will Ì_ b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17d39fdb-a237-4113-bf91-9cecf16f0882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='label'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['label'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421c6c01",
   "metadata": {},
   "source": [
    "## Task2 Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97fcb2a3-d9ef-45a6-bfc4-50c15182524e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5a75bfa-d91b-410f-b6fa-238bb07b4834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "      <th>content_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>life Goes until jurong point, crazy.. Availabl...</td>\n",
       "      <td>life Goes until jurong point crazy Available o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            content  \\\n",
       "0   ham  life Goes until jurong point, crazy.. Availabl...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       content_clean  \n",
       "0  life Goes until jurong point crazy Available o...  \n",
       "1                            Ok lar Joking wif u oni  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...  \n",
       "3        U dun say so early hor U c already then say  \n",
       "4  Nah I dont think he goes to usf he lives aroun...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punct(text):\n",
    "    text_nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return text_nopunct\n",
    "df['content_clean'] = df['content'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1752c9b0",
   "metadata": {},
   "source": [
    "## Task3 Tokeniation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ea9b8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e9d939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['content_tokenized'] = df['content_clean'].apply(lambda x: word_tokenize(x.lower()))\n",
    "df['content_tokenized'] = df['content_clean'].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1ff9257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "      <th>content_clean</th>\n",
       "      <th>content_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>life Goes until jurong point, crazy.. Availabl...</td>\n",
       "      <td>life Goes until jurong point crazy Available o...</td>\n",
       "      <td>[life, Goes, until, jurong, point, crazy, Avai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>[U, dun, say, so, early, hor, U, c, already, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[Nah, I, dont, think, he, goes, to, usf, he, l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            content  \\\n",
       "0   ham  life Goes until jurong point, crazy.. Availabl...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       content_clean  \\\n",
       "0  life Goes until jurong point crazy Available o...   \n",
       "1                            Ok lar Joking wif u oni   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3        U dun say so early hor U c already then say   \n",
       "4  Nah I dont think he goes to usf he lives aroun...   \n",
       "\n",
       "                                   content_tokenized  \n",
       "0  [life, Goes, until, jurong, point, crazy, Avai...  \n",
       "1                     [Ok, lar, Joking, wif, u, oni]  \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...  \n",
       "3  [U, dun, say, so, early, hor, U, c, already, t...  \n",
       "4  [Nah, I, dont, think, he, goes, to, usf, he, l...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe30acb4",
   "metadata": {},
   "source": [
    "## Task4 Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b46f6f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "print(stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5826283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenized_list):\n",
    "    text = [word for word in tokenized_list if word not in stopword]\n",
    "    return text\n",
    "df['content_nostop'] = df['content_tokenized'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f5a71a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "      <th>content_clean</th>\n",
       "      <th>content_tokenized</th>\n",
       "      <th>content_nostop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>life Goes until jurong point, crazy.. Availabl...</td>\n",
       "      <td>life Goes until jurong point crazy Available o...</td>\n",
       "      <td>[life, Goes, until, jurong, point, crazy, Avai...</td>\n",
       "      <td>[life, Goes, jurong, point, crazy, Available, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
       "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>[Free, entry, 2, wkly, comp, win, FA, Cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>[U, dun, say, so, early, hor, U, c, already, t...</td>\n",
       "      <td>[U, dun, say, early, hor, U, c, already, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[Nah, I, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>[Nah, I, dont, think, goes, usf, lives, around...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            content  \\\n",
       "0   ham  life Goes until jurong point, crazy.. Availabl...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       content_clean  \\\n",
       "0  life Goes until jurong point crazy Available o...   \n",
       "1                            Ok lar Joking wif u oni   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3        U dun say so early hor U c already then say   \n",
       "4  Nah I dont think he goes to usf he lives aroun...   \n",
       "\n",
       "                                   content_tokenized  \\\n",
       "0  [life, Goes, until, jurong, point, crazy, Avai...   \n",
       "1                     [Ok, lar, Joking, wif, u, oni]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [U, dun, say, so, early, hor, U, c, already, t...   \n",
       "4  [Nah, I, dont, think, he, goes, to, usf, he, l...   \n",
       "\n",
       "                                      content_nostop  \n",
       "0  [life, Goes, jurong, point, crazy, Available, ...  \n",
       "1                     [Ok, lar, Joking, wif, u, oni]  \n",
       "2  [Free, entry, 2, wkly, comp, win, FA, Cup, fin...  \n",
       "3      [U, dun, say, early, hor, U, c, already, say]  \n",
       "4  [Nah, I, dont, think, goes, usf, lives, around...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b006d9",
   "metadata": {},
   "source": [
    "## Task5 Stemming (porter stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a92384",
   "metadata": {},
   "source": [
    "###### Definition: Stemming is a crude heuristic process that removes word endings (suffixes) to reduce a word to its base form or root, which may not always be a valid word in the language.\n",
    "###### Approach: It uses simple rules like cutting off common endings (e.g., -ing, -ed, -ly).\n",
    "###### Output: The result is often not a valid word, but rather a stem. For example, \"playing\" may become \"play,\" but \"studies\" could become \"studi.\"\n",
    "###### Speed: Fast because it does not involve complex linguistic analysis.\n",
    "###### Accuracy: Lower accuracy since it doesn't consider the meaning or grammatical role of the word.\n",
    "###### Example:\n",
    "###### \"Caring\" → \"car\"\n",
    "###### \"Studies\" → \"studi\"\n",
    "###### Common algorithms: Porter Stemmer, Snowball Stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b16f2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "413e79e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "      <th>content_clean</th>\n",
       "      <th>content_tokenized</th>\n",
       "      <th>content_nostop</th>\n",
       "      <th>content_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>life Goes until jurong point, crazy.. Availabl...</td>\n",
       "      <td>life Goes until jurong point crazy Available o...</td>\n",
       "      <td>[life, Goes, until, jurong, point, crazy, Avai...</td>\n",
       "      <td>[life, Goes, jurong, point, crazy, Available, ...</td>\n",
       "      <td>[life, goe, jurong, point, crazi, avail, bugi,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
       "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joke, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>[Free, entry, 2, wkly, comp, win, FA, Cup, fin...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>[U, dun, say, so, early, hor, U, c, already, t...</td>\n",
       "      <td>[U, dun, say, early, hor, U, c, already, say]</td>\n",
       "      <td>[u, dun, say, earli, hor, u, c, alreadi, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[Nah, I, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>[Nah, I, dont, think, goes, usf, lives, around...</td>\n",
       "      <td>[nah, i, dont, think, goe, usf, live, around, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            content  \\\n",
       "0   ham  life Goes until jurong point, crazy.. Availabl...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       content_clean  \\\n",
       "0  life Goes until jurong point crazy Available o...   \n",
       "1                            Ok lar Joking wif u oni   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3        U dun say so early hor U c already then say   \n",
       "4  Nah I dont think he goes to usf he lives aroun...   \n",
       "\n",
       "                                   content_tokenized  \\\n",
       "0  [life, Goes, until, jurong, point, crazy, Avai...   \n",
       "1                     [Ok, lar, Joking, wif, u, oni]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [U, dun, say, so, early, hor, U, c, already, t...   \n",
       "4  [Nah, I, dont, think, he, goes, to, usf, he, l...   \n",
       "\n",
       "                                      content_nostop  \\\n",
       "0  [life, Goes, jurong, point, crazy, Available, ...   \n",
       "1                     [Ok, lar, Joking, wif, u, oni]   \n",
       "2  [Free, entry, 2, wkly, comp, win, FA, Cup, fin...   \n",
       "3      [U, dun, say, early, hor, U, c, already, say]   \n",
       "4  [Nah, I, dont, think, goes, usf, lives, around...   \n",
       "\n",
       "                                     content_stemmed  \n",
       "0  [life, goe, jurong, point, crazi, avail, bugi,...  \n",
       "1                       [ok, lar, joke, wif, u, oni]  \n",
       "2  [free, entri, 2, wkli, comp, win, fa, cup, fin...  \n",
       "3      [u, dun, say, earli, hor, u, c, alreadi, say]  \n",
       "4  [nah, i, dont, think, goe, usf, live, around, ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stemming(tokenized_text):\n",
    "    text = [ps.stem(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "df['content_stemmed'] = df['content_nostop'].apply(lambda x: stemming(x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ae3365",
   "metadata": {},
   "source": [
    "## Task6: Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcbc99f",
   "metadata": {},
   "source": [
    "###### Definition: Lemmatization reduces words to their lemma, which is the base or dictionary form of a word, ensuring the result is a valid word in the language.\n",
    "###### Approach: It considers the meaning and context of the word, requiring the word's part of speech (POS). For example, \"better\" is reduced to \"good\" (adjective), but \"am\" becomes \"be\" (verb).\n",
    "###### Output: The result is a valid word that makes sense in the language.\n",
    "###### Speed: Slower compared to stemming since it involves more complex linguistic processing.\n",
    "###### Accuracy: Higher accuracy due to understanding the word's context and meaning.\n",
    "###### Example:\n",
    "###### \"Caring\" → \"care\"\n",
    "###### \"Studies\" → \"study\"\n",
    "###### Common tools: WordNet Lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf4708e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20695fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "      <th>content_clean</th>\n",
       "      <th>content_tokenized</th>\n",
       "      <th>content_nostop</th>\n",
       "      <th>content_stemmed</th>\n",
       "      <th>content_lemmetized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>life Goes until jurong point, crazy.. Availabl...</td>\n",
       "      <td>life Goes until jurong point crazy Available o...</td>\n",
       "      <td>[life, Goes, until, jurong, point, crazy, Avai...</td>\n",
       "      <td>[life, Goes, jurong, point, crazy, Available, ...</td>\n",
       "      <td>[life, goe, jurong, point, crazi, avail, bugi,...</td>\n",
       "      <td>[life, Goes, jurong, point, crazy, Available, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
       "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joke, wif, u, oni]</td>\n",
       "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>[Free, entry, 2, wkly, comp, win, FA, Cup, fin...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, fin...</td>\n",
       "      <td>[Free, entry, 2, wkly, comp, win, FA, Cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>[U, dun, say, so, early, hor, U, c, already, t...</td>\n",
       "      <td>[U, dun, say, early, hor, U, c, already, say]</td>\n",
       "      <td>[u, dun, say, earli, hor, u, c, alreadi, say]</td>\n",
       "      <td>[U, dun, say, early, hor, U, c, already, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[Nah, I, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>[Nah, I, dont, think, goes, usf, lives, around...</td>\n",
       "      <td>[nah, i, dont, think, goe, usf, live, around, ...</td>\n",
       "      <td>[Nah, I, dont, think, go, usf, life, around, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            content  \\\n",
       "0   ham  life Goes until jurong point, crazy.. Availabl...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       content_clean  \\\n",
       "0  life Goes until jurong point crazy Available o...   \n",
       "1                            Ok lar Joking wif u oni   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3        U dun say so early hor U c already then say   \n",
       "4  Nah I dont think he goes to usf he lives aroun...   \n",
       "\n",
       "                                   content_tokenized  \\\n",
       "0  [life, Goes, until, jurong, point, crazy, Avai...   \n",
       "1                     [Ok, lar, Joking, wif, u, oni]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [U, dun, say, so, early, hor, U, c, already, t...   \n",
       "4  [Nah, I, dont, think, he, goes, to, usf, he, l...   \n",
       "\n",
       "                                      content_nostop  \\\n",
       "0  [life, Goes, jurong, point, crazy, Available, ...   \n",
       "1                     [Ok, lar, Joking, wif, u, oni]   \n",
       "2  [Free, entry, 2, wkly, comp, win, FA, Cup, fin...   \n",
       "3      [U, dun, say, early, hor, U, c, already, say]   \n",
       "4  [Nah, I, dont, think, goes, usf, lives, around...   \n",
       "\n",
       "                                     content_stemmed  \\\n",
       "0  [life, goe, jurong, point, crazi, avail, bugi,...   \n",
       "1                       [ok, lar, joke, wif, u, oni]   \n",
       "2  [free, entri, 2, wkli, comp, win, fa, cup, fin...   \n",
       "3      [u, dun, say, earli, hor, u, c, alreadi, say]   \n",
       "4  [nah, i, dont, think, goe, usf, live, around, ...   \n",
       "\n",
       "                                  content_lemmetized  \n",
       "0  [life, Goes, jurong, point, crazy, Available, ...  \n",
       "1                     [Ok, lar, Joking, wif, u, oni]  \n",
       "2  [Free, entry, 2, wkly, comp, win, FA, Cup, fin...  \n",
       "3      [U, dun, say, early, hor, U, c, already, say]  \n",
       "4  [Nah, I, dont, think, go, usf, life, around, t...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatizing(tokenized_text):\n",
    "    text = [wn.lemmatize(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "df['content_lemmetized'] = df['content_nostop'].apply(lambda x: lemmatizing(x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cdd48e",
   "metadata": {},
   "source": [
    "## Task7: Vectorizing (Count Vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764a9a9c",
   "metadata": {},
   "source": [
    "###### Vectorizing in Natural Language Processing (NLP) is the process of converting text data into numerical format so that machine learning models can understand and process it. Text data, in its raw form, consists of strings, which models cannot work with. Vectorization transforms this text into vectors, i.e., arrays of numbers, making it usable for algorithms.\n",
    "\n",
    "##### Why is Vectorizing Important?\n",
    "###### Machine learning algorithms need numbers: Algorithms can only process numerical data. Since text is inherently non-numeric, vectorizing converts text into a numeric format.\n",
    "###### Representation of text: Vectorization encodes information about the structure, meaning, and context of the text into vectors that models can use for predictions and analysis.\n",
    "###### Feature extraction: It allows the model to capture key features from the text such as word frequency, relationships between words, or semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81234294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f49e6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_content'] = df['content_lemmetized'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d90b183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "      <th>content_clean</th>\n",
       "      <th>content_tokenized</th>\n",
       "      <th>content_nostop</th>\n",
       "      <th>content_stemmed</th>\n",
       "      <th>content_lemmetized</th>\n",
       "      <th>cleaned_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>life Goes until jurong point, crazy.. Availabl...</td>\n",
       "      <td>life Goes until jurong point crazy Available o...</td>\n",
       "      <td>[life, Goes, until, jurong, point, crazy, Avai...</td>\n",
       "      <td>[life, Goes, jurong, point, crazy, Available, ...</td>\n",
       "      <td>[life, goe, jurong, point, crazi, avail, bugi,...</td>\n",
       "      <td>[life, Goes, jurong, point, crazy, Available, ...</td>\n",
       "      <td>life Goes jurong point crazy Available bugis n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
       "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
       "      <td>[ok, lar, joke, wif, u, oni]</td>\n",
       "      <td>[Ok, lar, Joking, wif, u, oni]</td>\n",
       "      <td>Ok lar Joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[Free, entry, in, 2, a, wkly, comp, to, win, F...</td>\n",
       "      <td>[Free, entry, 2, wkly, comp, win, FA, Cup, fin...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, fin...</td>\n",
       "      <td>[Free, entry, 2, wkly, comp, win, FA, Cup, fin...</td>\n",
       "      <td>Free entry 2 wkly comp win FA Cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>[U, dun, say, so, early, hor, U, c, already, t...</td>\n",
       "      <td>[U, dun, say, early, hor, U, c, already, say]</td>\n",
       "      <td>[u, dun, say, earli, hor, u, c, alreadi, say]</td>\n",
       "      <td>[U, dun, say, early, hor, U, c, already, say]</td>\n",
       "      <td>U dun say early hor U c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[Nah, I, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>[Nah, I, dont, think, goes, usf, lives, around...</td>\n",
       "      <td>[nah, i, dont, think, goe, usf, live, around, ...</td>\n",
       "      <td>[Nah, I, dont, think, go, usf, life, around, t...</td>\n",
       "      <td>Nah I dont think go usf life around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            content  \\\n",
       "0   ham  life Goes until jurong point, crazy.. Availabl...   \n",
       "1   ham                      Ok lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       content_clean  \\\n",
       "0  life Goes until jurong point crazy Available o...   \n",
       "1                            Ok lar Joking wif u oni   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3        U dun say so early hor U c already then say   \n",
       "4  Nah I dont think he goes to usf he lives aroun...   \n",
       "\n",
       "                                   content_tokenized  \\\n",
       "0  [life, Goes, until, jurong, point, crazy, Avai...   \n",
       "1                     [Ok, lar, Joking, wif, u, oni]   \n",
       "2  [Free, entry, in, 2, a, wkly, comp, to, win, F...   \n",
       "3  [U, dun, say, so, early, hor, U, c, already, t...   \n",
       "4  [Nah, I, dont, think, he, goes, to, usf, he, l...   \n",
       "\n",
       "                                      content_nostop  \\\n",
       "0  [life, Goes, jurong, point, crazy, Available, ...   \n",
       "1                     [Ok, lar, Joking, wif, u, oni]   \n",
       "2  [Free, entry, 2, wkly, comp, win, FA, Cup, fin...   \n",
       "3      [U, dun, say, early, hor, U, c, already, say]   \n",
       "4  [Nah, I, dont, think, goes, usf, lives, around...   \n",
       "\n",
       "                                     content_stemmed  \\\n",
       "0  [life, goe, jurong, point, crazi, avail, bugi,...   \n",
       "1                       [ok, lar, joke, wif, u, oni]   \n",
       "2  [free, entri, 2, wkli, comp, win, fa, cup, fin...   \n",
       "3      [u, dun, say, earli, hor, u, c, alreadi, say]   \n",
       "4  [nah, i, dont, think, goe, usf, live, around, ...   \n",
       "\n",
       "                                  content_lemmetized  \\\n",
       "0  [life, Goes, jurong, point, crazy, Available, ...   \n",
       "1                     [Ok, lar, Joking, wif, u, oni]   \n",
       "2  [Free, entry, 2, wkly, comp, win, FA, Cup, fin...   \n",
       "3      [U, dun, say, early, hor, U, c, already, say]   \n",
       "4  [Nah, I, dont, think, go, usf, life, around, t...   \n",
       "\n",
       "                                     cleaned_content  \n",
       "0  life Goes jurong point crazy Available bugis n...  \n",
       "1                            Ok lar Joking wif u oni  \n",
       "2  Free entry 2 wkly comp win FA Cup final tkts 2...  \n",
       "3                U dun say early hor U c already say  \n",
       "4         Nah I dont think go usf life around though  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e7c14e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 243)\n",
      "['09061701461 claim' '100 20000' '100000 prize' '11 month' '12 hour'\n",
      " '150 rcv' '150pday 6days' '16 tsandcs' '20000 pound' '2005 text'\n",
      " '21st may' '4txtì¼120 poboxox36504w45wq' '6days 16' '81010 tc' '87077 eg'\n",
      " '87077 trywales' '87121 receive' '87575 cost' '900 prize' 'aid patent'\n",
      " 'already say' 'amore wat' 'anymore tonight' 'apply 08452810075over18s'\n",
      " 'apply reply' 'around though' 'as per' 'as valued' 'available bugis'\n",
      " 'back id' 'blessing time' 'breather promise' 'brother like' 'buffet cine'\n",
      " 'bugis great' 'call 09061701461' 'call the' 'callers press'\n",
      " 'callertune callers' 'camera free' 'cash from' 'chance win' 'chgs send'\n",
      " 'cine got' 'claim call' 'claim code' 'claim no' 'click httpwap'\n",
      " 'click wap' 'co free' 'code kl341' 'colour mobile' 'comp win'\n",
      " 'copy friend' 'cost 150pday' 'crazy available' 'credit click'\n",
      " 'cried enough' 'csh11 send' 'cup final' 'customer selected'\n",
      " 'darling week' 'date on' 'dont miss' 'dont think' 'dont want' 'dun say'\n",
      " 'early hor' 'eg england' 'eh remember' 'england 87077'\n",
      " 'england macedonia' 'enough today' 'entitled update' 'entry questionstd'\n",
      " 'entry wkly' 'even brother' 'fa 87121' 'fa cup' 'feel thatåõs'\n",
      " 'final tkts' 'fine thatåõs' 'free 08002986030' 'free call' 'free entry'\n",
      " 'free membership' 'freemsg hey' 'friend callertune' 'from 100'\n",
      " 'fulfil promise' 'fun still' 'go usf' 'goalsteam news' 'goes jurong'\n",
      " 'gonna home' 'got amore' 'granted fulfil' 'great world' 'had mobile'\n",
      " 'have date' 'he naughty' 'help granted' 'hey darling' 'hl info'\n",
      " 'home soon' 'hor already' 'httpwap xxxmobilemovieclubcomnqjkgighjjgcbl'\n",
      " 'id like' 'im gonna' 'ive cried' 'ive searching' 'jackpot txt'\n",
      " 'joking wif' 'jurong point' 'kim watching' 'kl341 valid' 'la buffet'\n",
      " 'lar joking' 'latest colour' 'lccltd pobox' 'life around' 'life goes'\n",
      " 'like aid' 'like fun' 'like speak' 'link next' 'macedonia dont'\n",
      " 'make wet' 'may 2005' 'melle melle' 'melle oru' 'membership 100000'\n",
      " 'message click' 'minnaminunginte nurungu' 'miss goalsteam' 'mobile 11'\n",
      " 'mobile camera' 'mobile update' 'month entitled' 'nah dont' 'name yes'\n",
      " 'national team' 'naughty make' 'network customer' 'news txt' 'next txt'\n",
      " 'no 81010' 'nurungu vettam' 'oh kim' 'ok lar' 'ok xxx' 'on sunday'\n",
      " 'oru minnaminunginte' 'per request' 'pobox 4403ldnw1a7rw18'\n",
      " 'poboxox36504w45wq 16' 'point crazy' 'pound txt' 'press copy'\n",
      " 'prize jackpot' 'prize reward' 'promise wont' 'promise you'\n",
      " 'questionstd txt' 'ratetcs apply' 'receive entry' 'receivea 900'\n",
      " 'remember spell' 'reply hl' 'request melle' 'reward to' 'right word'\n",
      " 'say early' 'scotland 4txtì¼120' 'searching right' 'selected receivea'\n",
      " 'send 150' 'send 87575' 'set callertune' 'six chance' 'soon dont'\n",
      " 'speak they' 'spell name' 'std chgs' 'still tb' 'stuff anymore'\n",
      " 'sunday with' 'take help' 'talk stuff' 'tb ok' 'tc wwwdbuknet'\n",
      " 'team 87077' 'text fa' 'thank breather' 'thatåõs way' 'the mobile'\n",
      " 'they treat' 'think go' 'tkts 21st' 'to claim' 'to use' 'tonight ive'\n",
      " 'treat like' 'trywales scotland' 'tsandcs apply' 'txt csh11'\n",
      " 'txt message' 'txt ratetcs' 'txt ur' 'txt word' 'update co'\n",
      " 'update latest' 'ur national' 'urgent you' 'use credit' 'usf life'\n",
      " 'valid 12' 'valued network' 'vettam set' 'want talk' 'wap link'\n",
      " 'way feel' 'way gota' 'week free' 'week word' 'wif oni' 'win cash'\n",
      " 'win fa' 'winner as' 'with will' 'wkly comp' 'wonderful blessing'\n",
      " 'wont take' 'word back' 'word claim' 'word thank' 'world la'\n",
      " 'wwwdbuknet lccltd' 'xxx std' 'xxxmobilemovieclub to' 'yes he' 'you week'\n",
      " 'you wonderful']\n"
     ]
    }
   ],
   "source": [
    "# this task is for sample displaying\n",
    "data_sample = df[0:20]\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(2,2))\n",
    "result_vector = cv.fit_transform(data_sample['cleaned_content'])\n",
    "print(result_vector.shape)\n",
    "print(cv.get_feature_names_out())\n",
    "feature_names = list(cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac6cc5db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['09061701461 claim',\n",
       " '100 20000',\n",
       " '100000 prize',\n",
       " '11 month',\n",
       " '12 hour',\n",
       " '150 rcv',\n",
       " '150pday 6days',\n",
       " '16 tsandcs',\n",
       " '20000 pound',\n",
       " '2005 text',\n",
       " '21st may',\n",
       " '4txtì¼120 poboxox36504w45wq',\n",
       " '6days 16',\n",
       " '81010 tc',\n",
       " '87077 eg',\n",
       " '87077 trywales',\n",
       " '87121 receive',\n",
       " '87575 cost',\n",
       " '900 prize',\n",
       " 'aid patent',\n",
       " 'already say',\n",
       " 'amore wat',\n",
       " 'anymore tonight',\n",
       " 'apply 08452810075over18s',\n",
       " 'apply reply',\n",
       " 'around though',\n",
       " 'as per',\n",
       " 'as valued',\n",
       " 'available bugis',\n",
       " 'back id',\n",
       " 'blessing time',\n",
       " 'breather promise',\n",
       " 'brother like',\n",
       " 'buffet cine',\n",
       " 'bugis great',\n",
       " 'call 09061701461',\n",
       " 'call the',\n",
       " 'callers press',\n",
       " 'callertune callers',\n",
       " 'camera free',\n",
       " 'cash from',\n",
       " 'chance win',\n",
       " 'chgs send',\n",
       " 'cine got',\n",
       " 'claim call',\n",
       " 'claim code',\n",
       " 'claim no',\n",
       " 'click httpwap',\n",
       " 'click wap',\n",
       " 'co free',\n",
       " 'code kl341',\n",
       " 'colour mobile',\n",
       " 'comp win',\n",
       " 'copy friend',\n",
       " 'cost 150pday',\n",
       " 'crazy available',\n",
       " 'credit click',\n",
       " 'cried enough',\n",
       " 'csh11 send',\n",
       " 'cup final',\n",
       " 'customer selected',\n",
       " 'darling week',\n",
       " 'date on',\n",
       " 'dont miss',\n",
       " 'dont think',\n",
       " 'dont want',\n",
       " 'dun say',\n",
       " 'early hor',\n",
       " 'eg england',\n",
       " 'eh remember',\n",
       " 'england 87077',\n",
       " 'england macedonia',\n",
       " 'enough today',\n",
       " 'entitled update',\n",
       " 'entry questionstd',\n",
       " 'entry wkly',\n",
       " 'even brother',\n",
       " 'fa 87121',\n",
       " 'fa cup',\n",
       " 'feel thatåõs',\n",
       " 'final tkts',\n",
       " 'fine thatåõs',\n",
       " 'free 08002986030',\n",
       " 'free call',\n",
       " 'free entry',\n",
       " 'free membership',\n",
       " 'freemsg hey',\n",
       " 'friend callertune',\n",
       " 'from 100',\n",
       " 'fulfil promise',\n",
       " 'fun still',\n",
       " 'go usf',\n",
       " 'goalsteam news',\n",
       " 'goes jurong',\n",
       " 'gonna home',\n",
       " 'got amore',\n",
       " 'granted fulfil',\n",
       " 'great world',\n",
       " 'had mobile',\n",
       " 'have date',\n",
       " 'he naughty',\n",
       " 'help granted',\n",
       " 'hey darling',\n",
       " 'hl info',\n",
       " 'home soon',\n",
       " 'hor already',\n",
       " 'httpwap xxxmobilemovieclubcomnqjkgighjjgcbl',\n",
       " 'id like',\n",
       " 'im gonna',\n",
       " 'ive cried',\n",
       " 'ive searching',\n",
       " 'jackpot txt',\n",
       " 'joking wif',\n",
       " 'jurong point',\n",
       " 'kim watching',\n",
       " 'kl341 valid',\n",
       " 'la buffet',\n",
       " 'lar joking',\n",
       " 'latest colour',\n",
       " 'lccltd pobox',\n",
       " 'life around',\n",
       " 'life goes',\n",
       " 'like aid',\n",
       " 'like fun',\n",
       " 'like speak',\n",
       " 'link next',\n",
       " 'macedonia dont',\n",
       " 'make wet',\n",
       " 'may 2005',\n",
       " 'melle melle',\n",
       " 'melle oru',\n",
       " 'membership 100000',\n",
       " 'message click',\n",
       " 'minnaminunginte nurungu',\n",
       " 'miss goalsteam',\n",
       " 'mobile 11',\n",
       " 'mobile camera',\n",
       " 'mobile update',\n",
       " 'month entitled',\n",
       " 'nah dont',\n",
       " 'name yes',\n",
       " 'national team',\n",
       " 'naughty make',\n",
       " 'network customer',\n",
       " 'news txt',\n",
       " 'next txt',\n",
       " 'no 81010',\n",
       " 'nurungu vettam',\n",
       " 'oh kim',\n",
       " 'ok lar',\n",
       " 'ok xxx',\n",
       " 'on sunday',\n",
       " 'oru minnaminunginte',\n",
       " 'per request',\n",
       " 'pobox 4403ldnw1a7rw18',\n",
       " 'poboxox36504w45wq 16',\n",
       " 'point crazy',\n",
       " 'pound txt',\n",
       " 'press copy',\n",
       " 'prize jackpot',\n",
       " 'prize reward',\n",
       " 'promise wont',\n",
       " 'promise you',\n",
       " 'questionstd txt',\n",
       " 'ratetcs apply',\n",
       " 'receive entry',\n",
       " 'receivea 900',\n",
       " 'remember spell',\n",
       " 'reply hl',\n",
       " 'request melle',\n",
       " 'reward to',\n",
       " 'right word',\n",
       " 'say early',\n",
       " 'scotland 4txtì¼120',\n",
       " 'searching right',\n",
       " 'selected receivea',\n",
       " 'send 150',\n",
       " 'send 87575',\n",
       " 'set callertune',\n",
       " 'six chance',\n",
       " 'soon dont',\n",
       " 'speak they',\n",
       " 'spell name',\n",
       " 'std chgs',\n",
       " 'still tb',\n",
       " 'stuff anymore',\n",
       " 'sunday with',\n",
       " 'take help',\n",
       " 'talk stuff',\n",
       " 'tb ok',\n",
       " 'tc wwwdbuknet',\n",
       " 'team 87077',\n",
       " 'text fa',\n",
       " 'thank breather',\n",
       " 'thatåõs way',\n",
       " 'the mobile',\n",
       " 'they treat',\n",
       " 'think go',\n",
       " 'tkts 21st',\n",
       " 'to claim',\n",
       " 'to use',\n",
       " 'tonight ive',\n",
       " 'treat like',\n",
       " 'trywales scotland',\n",
       " 'tsandcs apply',\n",
       " 'txt csh11',\n",
       " 'txt message',\n",
       " 'txt ratetcs',\n",
       " 'txt ur',\n",
       " 'txt word',\n",
       " 'update co',\n",
       " 'update latest',\n",
       " 'ur national',\n",
       " 'urgent you',\n",
       " 'use credit',\n",
       " 'usf life',\n",
       " 'valid 12',\n",
       " 'valued network',\n",
       " 'vettam set',\n",
       " 'want talk',\n",
       " 'wap link',\n",
       " 'way feel',\n",
       " 'way gota',\n",
       " 'week free',\n",
       " 'week word',\n",
       " 'wif oni',\n",
       " 'win cash',\n",
       " 'win fa',\n",
       " 'winner as',\n",
       " 'with will',\n",
       " 'wkly comp',\n",
       " 'wonderful blessing',\n",
       " 'wont take',\n",
       " 'word back',\n",
       " 'word claim',\n",
       " 'word thank',\n",
       " 'world la',\n",
       " 'wwwdbuknet lccltd',\n",
       " 'xxx std',\n",
       " 'xxxmobilemovieclub to',\n",
       " 'yes he',\n",
       " 'you week',\n",
       " 'you wonderful']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbcdfb0",
   "metadata": {},
   "source": [
    "## Task8: Creating Vector DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25c3ac69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>09061701461 claim</th>\n",
       "      <th>100 20000</th>\n",
       "      <th>100000 prize</th>\n",
       "      <th>11 month</th>\n",
       "      <th>12 hour</th>\n",
       "      <th>150 rcv</th>\n",
       "      <th>150pday 6days</th>\n",
       "      <th>16 tsandcs</th>\n",
       "      <th>20000 pound</th>\n",
       "      <th>2005 text</th>\n",
       "      <th>...</th>\n",
       "      <th>word back</th>\n",
       "      <th>word claim</th>\n",
       "      <th>word thank</th>\n",
       "      <th>world la</th>\n",
       "      <th>wwwdbuknet lccltd</th>\n",
       "      <th>xxx std</th>\n",
       "      <th>xxxmobilemovieclub to</th>\n",
       "      <th>yes he</th>\n",
       "      <th>you week</th>\n",
       "      <th>you wonderful</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 243 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    09061701461 claim  100 20000  100000 prize  11 month  12 hour  150 rcv  \\\n",
       "0                   0          0             0         0        0        0   \n",
       "1                   0          0             0         0        0        0   \n",
       "2                   0          0             0         0        0        0   \n",
       "3                   0          0             0         0        0        0   \n",
       "4                   0          0             0         0        0        0   \n",
       "5                   0          0             0         0        0        1   \n",
       "6                   0          0             0         0        0        0   \n",
       "7                   0          0             0         0        0        0   \n",
       "8                   1          0             0         0        1        0   \n",
       "9                   0          0             0         1        0        0   \n",
       "10                  0          0             0         0        0        0   \n",
       "11                  0          1             0         0        0        0   \n",
       "12                  0          0             1         0        0        0   \n",
       "13                  0          0             0         0        0        0   \n",
       "14                  0          0             0         0        0        0   \n",
       "15                  0          0             0         0        0        0   \n",
       "16                  0          0             0         0        0        0   \n",
       "17                  0          0             0         0        0        0   \n",
       "18                  0          0             0         0        0        0   \n",
       "19                  0          0             0         0        0        0   \n",
       "\n",
       "    150pday 6days  16 tsandcs  20000 pound  2005 text  ...  word back  \\\n",
       "0               0           0            0          0  ...          0   \n",
       "1               0           0            0          0  ...          0   \n",
       "2               0           0            0          1  ...          0   \n",
       "3               0           0            0          0  ...          0   \n",
       "4               0           0            0          0  ...          0   \n",
       "5               0           0            0          0  ...          1   \n",
       "6               0           0            0          0  ...          0   \n",
       "7               0           0            0          0  ...          0   \n",
       "8               0           0            0          0  ...          0   \n",
       "9               0           0            0          0  ...          0   \n",
       "10              0           0            0          0  ...          0   \n",
       "11              1           1            1          0  ...          0   \n",
       "12              0           0            0          0  ...          0   \n",
       "13              0           0            0          0  ...          0   \n",
       "14              0           0            0          0  ...          0   \n",
       "15              0           0            0          0  ...          0   \n",
       "16              0           0            0          0  ...          0   \n",
       "17              0           0            0          0  ...          0   \n",
       "18              0           0            0          0  ...          0   \n",
       "19              0           0            0          0  ...          0   \n",
       "\n",
       "    word claim  word thank  world la  wwwdbuknet lccltd  xxx std  \\\n",
       "0            0           0         1                  0        0   \n",
       "1            0           0         0                  0        0   \n",
       "2            0           0         0                  0        0   \n",
       "3            0           0         0                  0        0   \n",
       "4            0           0         0                  0        0   \n",
       "5            0           0         0                  0        1   \n",
       "6            0           0         0                  0        0   \n",
       "7            0           0         0                  0        0   \n",
       "8            0           0         0                  0        0   \n",
       "9            0           0         0                  0        0   \n",
       "10           0           0         0                  0        0   \n",
       "11           0           0         0                  0        0   \n",
       "12           1           0         0                  1        0   \n",
       "13           0           1         0                  0        0   \n",
       "14           0           0         0                  0        0   \n",
       "15           0           0         0                  0        0   \n",
       "16           0           0         0                  0        0   \n",
       "17           0           0         0                  0        0   \n",
       "18           0           0         0                  0        0   \n",
       "19           0           0         0                  0        0   \n",
       "\n",
       "    xxxmobilemovieclub to  yes he  you week  you wonderful  \n",
       "0                       0       0         0              0  \n",
       "1                       0       0         0              0  \n",
       "2                       0       0         0              0  \n",
       "3                       0       0         0              0  \n",
       "4                       0       0         0              0  \n",
       "5                       0       0         0              0  \n",
       "6                       0       0         0              0  \n",
       "7                       0       0         0              0  \n",
       "8                       0       0         0              0  \n",
       "9                       0       0         0              0  \n",
       "10                      0       0         0              0  \n",
       "11                      0       0         0              0  \n",
       "12                      0       0         1              0  \n",
       "13                      0       0         0              1  \n",
       "14                      0       0         0              0  \n",
       "15                      1       0         0              0  \n",
       "16                      0       0         0              0  \n",
       "17                      0       1         0              0  \n",
       "18                      0       0         0              0  \n",
       "19                      0       0         0              0  \n",
       "\n",
       "[20 rows x 243 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_vector_df = pd.DataFrame(result_vector.toarray())\n",
    "result_vector_df.columns = cv.get_feature_names_out()\n",
    "result_vector_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce01311",
   "metadata": {},
   "source": [
    "## Task9: Text Classification Model Building (Tfidf Vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94d8f2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
      "\twith 47142 stored elements and shape (5572, 5000)>\n",
      "  Coords\tValues\n",
      "  (0, 2535)\t0.23812775718062085\n",
      "  (0, 3160)\t0.27419805492029276\n",
      "  (0, 1445)\t0.30634600514871935\n",
      "  (0, 814)\t0.30080318466792305\n",
      "  (0, 1160)\t0.33418379245270713\n",
      "  (0, 2064)\t0.22017182980113595\n",
      "  (0, 4897)\t0.268823617782537\n",
      "  (0, 2475)\t0.33418379245270713\n",
      "  (0, 1159)\t0.3776195862770545\n",
      "  (0, 1318)\t0.33418379245270713\n",
      "  (0, 2049)\t0.18583407427351364\n",
      "  (0, 4782)\t0.22554626693889174\n",
      "  (1, 2990)\t0.2741803362458879\n",
      "  (1, 2489)\t0.4080505634471753\n",
      "  (1, 2396)\t0.5233273701797004\n",
      "  (1, 4848)\t0.4313385731102942\n",
      "  (1, 3004)\t0.5462557824449175\n",
      "  (2, 1926)\t0.11509704434096285\n",
      "  (2, 1737)\t0.3573358770638972\n",
      "  (2, 4877)\t0.1891395742846073\n",
      "  (2, 1372)\t0.1952406846233997\n",
      "  (2, 4856)\t0.1462510447348445\n",
      "  (2, 1804)\t0.46628848811198675\n",
      "  (2, 1464)\t0.20310638388373506\n",
      "  (2, 1859)\t0.18267631774267226\n",
      "  :\t:\n",
      "  (5568, 2194)\t0.3316880414431011\n",
      "  (5568, 4854)\t0.4231274093989247\n",
      "  (5568, 2033)\t0.32945575251582926\n",
      "  (5568, 1922)\t0.5099553392952768\n",
      "  (5568, 1748)\t0.5851074240149943\n",
      "  (5569, 2816)\t0.5154637494387073\n",
      "  (5569, 4341)\t0.5916968433151221\n",
      "  (5569, 4006)\t0.6198322100581006\n",
      "  (5570, 1926)\t0.19044204530581554\n",
      "  (5570, 4803)\t0.2181325542238668\n",
      "  (5570, 2256)\t0.27728454706096667\n",
      "  (5570, 2541)\t0.18944771817310557\n",
      "  (5570, 4463)\t0.22322989523539108\n",
      "  (5570, 2911)\t0.24949057659553536\n",
      "  (5570, 4031)\t0.24457597474469323\n",
      "  (5570, 1715)\t0.2898446244175551\n",
      "  (5570, 1985)\t0.32698583661683644\n",
      "  (5570, 1175)\t0.3360641915955829\n",
      "  (5570, 2084)\t0.24949057659553536\n",
      "  (5570, 2317)\t0.3413927035923167\n",
      "  (5570, 494)\t0.3857654811514526\n",
      "  (5571, 2871)\t0.4503310520484134\n",
      "  (5571, 2351)\t0.373926650716413\n",
      "  (5571, 4594)\t0.4976993678212746\n",
      "  (5571, 3588)\t0.6400594837320032\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "#Prepare data and labels\n",
    "X = df['cleaned_content']\n",
    "y = df['label']\n",
    "\n",
    "#TfId Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
    "print(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4050e539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LinearSVC<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.svm.LinearSVC.html\">?<span>Documentation for LinearSVC</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LinearSVC(random_state=42)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC(random_state=42)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "#Split data into train and test sets\n",
    "X_train, X_test,y_train,y_test = train_test_split(X_tfidf, y,test_size=0.2,random_state=42)\n",
    "\n",
    "#Initialize SVM Classifier\n",
    "svm = LinearSVC(random_state=42)\n",
    "\n",
    "#train the classifier\n",
    "svm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2335f571",
   "metadata": {},
   "source": [
    "## Task10: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "75b3d2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9802690582959641\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99       965\n",
      "        spam       0.98      0.87      0.92       150\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.98      0.93      0.96      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#predictions\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "#classification report\n",
    "report = classification_report(y_test,y_pred)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
