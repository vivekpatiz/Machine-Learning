{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f81e9de-4b5e-4826-889a-4a78775d7c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vivek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Error loading stopwards: Package 'stopwards' not found in\n",
      "[nltk_data]     index\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vivek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\vivek\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwards')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.data.path.append(r'C:\\Users\\USER\\AppData\\Roaming\\nltk_data')\n",
    "import pandas as pd\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4e4ff50-aace-41a3-84a9-9a314e081c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go dancing until jurong point, crazy.. Availab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok better lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2\n",
       "0   ham  Go dancing until jurong point, crazy.. Availab...\n",
       "1   ham               Ok better lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"spam_sms.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9933843-f153-470a-86e1-0305d65d4a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['label','content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd12b538-d770-4d1f-8b23-17c293d43239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go dancing until jurong point, crazy.. Availab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok better lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will �_ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                            content\n",
       "0      ham  Go dancing until jurong point, crazy.. Availab...\n",
       "1      ham               Ok better lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham              Will �_ b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17d39fdb-a237-4113-bf91-9cecf16f0882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['label'].value_counts().plot(kind='bar')\n",
    "# df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421c6c01",
   "metadata": {},
   "source": [
    "## Task2 Removing Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97fcb2a3-d9ef-45a6-bfc4-50c15182524e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import string\n",
    "# string.punctuation\n",
    "from string import punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5a75bfa-d91b-410f-b6fa-238bb07b4834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "      <th>content_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go dancing until jurong point, crazy.. Availab...</td>\n",
       "      <td>Go dancing until jurong point crazy Available ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok better lar... Joking wif u oni...</td>\n",
       "      <td>Ok better lar Joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            content  \\\n",
       "0   ham  Go dancing until jurong point, crazy.. Availab...   \n",
       "1   ham               Ok better lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       content_clean  \n",
       "0  Go dancing until jurong point crazy Available ...  \n",
       "1                     Ok better lar Joking wif u oni  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...  \n",
       "3        U dun say so early hor U c already then say  \n",
       "4  Nah I dont think he goes to usf he lives aroun...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_punct(text):\n",
    "    text_nopunct = \"\".join([char for char in text if char not in punctuation])\n",
    "    return text_nopunct\n",
    "df['content_clean'] = df['content'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "# df['content_clean'] = df['content'].str.replace(r\"[{}]\".format(punctuation), \"\", regex=True)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1752c9b0",
   "metadata": {},
   "source": [
    "## Task3 Tokeniation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ea9b8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e9d939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content_tokenized'] = df['content_clean'].apply(lambda x: word_tokenize(x.lower()))\n",
    "# df['content_tokenized'] = df['content_clean'].apply(lambda x: x.lower().split())\n",
    "\n",
    "# df['content_tokenized'] = df['content_clean'].str.lower().str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1ff9257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "      <th>content_clean</th>\n",
       "      <th>content_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go dancing until jurong point, crazy.. Availab...</td>\n",
       "      <td>Go dancing until jurong point crazy Available ...</td>\n",
       "      <td>[go, dancing, until, jurong, point, crazy, ava...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok better lar... Joking wif u oni...</td>\n",
       "      <td>Ok better lar Joking wif u oni</td>\n",
       "      <td>[ok, better, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            content  \\\n",
       "0   ham  Go dancing until jurong point, crazy.. Availab...   \n",
       "1   ham               Ok better lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       content_clean  \\\n",
       "0  Go dancing until jurong point crazy Available ...   \n",
       "1                     Ok better lar Joking wif u oni   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3        U dun say so early hor U c already then say   \n",
       "4  Nah I dont think he goes to usf he lives aroun...   \n",
       "\n",
       "                                   content_tokenized  \n",
       "0  [go, dancing, until, jurong, point, crazy, ava...  \n",
       "1             [ok, better, lar, joking, wif, u, oni]  \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...  \n",
       "3  [u, dun, say, so, early, hor, u, c, already, t...  \n",
       "4  [nah, i, dont, think, he, goes, to, usf, he, l...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe30acb4",
   "metadata": {},
   "source": [
    "## Task4 Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b46f6f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vivek\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "print(stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5826283e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(tokenized_list):\n",
    "    text = [word for word in tokenized_list if word not in stopword]\n",
    "    return text\n",
    "df['content_nostop'] = df['content_tokenized'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "# df['content_nostop'] = df['content_tokenized'].apply(lambda tokens: [x for x in tokens if x not in stopword])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f5a71a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "      <th>content_clean</th>\n",
       "      <th>content_tokenized</th>\n",
       "      <th>content_nostop</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go dancing until jurong point, crazy.. Availab...</td>\n",
       "      <td>Go dancing until jurong point crazy Available ...</td>\n",
       "      <td>[go, dancing, until, jurong, point, crazy, ava...</td>\n",
       "      <td>[go, dancing, jurong, point, crazy, available,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok better lar... Joking wif u oni...</td>\n",
       "      <td>Ok better lar Joking wif u oni</td>\n",
       "      <td>[ok, better, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, better, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            content  \\\n",
       "0   ham  Go dancing until jurong point, crazy.. Availab...   \n",
       "1   ham               Ok better lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       content_clean  \\\n",
       "0  Go dancing until jurong point crazy Available ...   \n",
       "1                     Ok better lar Joking wif u oni   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3        U dun say so early hor U c already then say   \n",
       "4  Nah I dont think he goes to usf he lives aroun...   \n",
       "\n",
       "                                   content_tokenized  \\\n",
       "0  [go, dancing, until, jurong, point, crazy, ava...   \n",
       "1             [ok, better, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3  [u, dun, say, so, early, hor, u, c, already, t...   \n",
       "4  [nah, i, dont, think, he, goes, to, usf, he, l...   \n",
       "\n",
       "                                      content_nostop  \n",
       "0  [go, dancing, jurong, point, crazy, available,...  \n",
       "1             [ok, better, lar, joking, wif, u, oni]  \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, fin...  \n",
       "3      [u, dun, say, early, hor, u, c, already, say]  \n",
       "4  [nah, dont, think, goes, usf, lives, around, t...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7182f4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<demo>hgdfytg</demo> hfytfd'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "text = '<demo>hgdfytg</demo> hfytfd'\n",
    "text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b006d9",
   "metadata": {},
   "source": [
    "## Task5 Stemming (porter stemming)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a92384",
   "metadata": {},
   "source": [
    "###### Definition: Stemming is a crude heuristic process that removes word endings (suffixes) to reduce a word to its base form or root, which may not always be a valid word in the language.\n",
    "###### Approach: It uses simple rules like cutting off common endings (e.g., -ing, -ed, -ly).\n",
    "###### Output: The result is often not a valid word, but rather a stem. For example, \"playing\" may become \"play,\" but \"studies\" could become \"studi.\"\n",
    "###### Speed: Fast because it does not involve complex linguistic analysis.\n",
    "###### Accuracy: Lower accuracy since it doesn't consider the meaning or grammatical role of the word.\n",
    "###### Example:\n",
    "###### \"Caring\" → \"car\"\n",
    "###### \"Studies\" → \"studi\"\n",
    "###### Common algorithms: Porter Stemmer, Snowball Stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b16f2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = nltk.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "413e79e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "      <th>content_clean</th>\n",
       "      <th>content_tokenized</th>\n",
       "      <th>content_nostop</th>\n",
       "      <th>content_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go dancing until jurong point, crazy.. Availab...</td>\n",
       "      <td>Go dancing until jurong point crazy Available ...</td>\n",
       "      <td>[go, dancing, until, jurong, point, crazy, ava...</td>\n",
       "      <td>[go, dancing, jurong, point, crazy, available,...</td>\n",
       "      <td>[go, danc, jurong, point, crazi, avail, bugi, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok better lar... Joking wif u oni...</td>\n",
       "      <td>Ok better lar Joking wif u oni</td>\n",
       "      <td>[ok, better, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, better, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, better, lar, joke, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "      <td>[u, dun, say, earli, hor, u, c, alreadi, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, t...</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, tho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            content  \\\n",
       "0   ham  Go dancing until jurong point, crazy.. Availab...   \n",
       "1   ham               Ok better lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       content_clean  \\\n",
       "0  Go dancing until jurong point crazy Available ...   \n",
       "1                     Ok better lar Joking wif u oni   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3        U dun say so early hor U c already then say   \n",
       "4  Nah I dont think he goes to usf he lives aroun...   \n",
       "\n",
       "                                   content_tokenized  \\\n",
       "0  [go, dancing, until, jurong, point, crazy, ava...   \n",
       "1             [ok, better, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3  [u, dun, say, so, early, hor, u, c, already, t...   \n",
       "4  [nah, i, dont, think, he, goes, to, usf, he, l...   \n",
       "\n",
       "                                      content_nostop  \\\n",
       "0  [go, dancing, jurong, point, crazy, available,...   \n",
       "1             [ok, better, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "3      [u, dun, say, early, hor, u, c, already, say]   \n",
       "4  [nah, dont, think, goes, usf, lives, around, t...   \n",
       "\n",
       "                                     content_stemmed  \n",
       "0  [go, danc, jurong, point, crazi, avail, bugi, ...  \n",
       "1               [ok, better, lar, joke, wif, u, oni]  \n",
       "2  [free, entri, 2, wkli, comp, win, fa, cup, fin...  \n",
       "3      [u, dun, say, earli, hor, u, c, alreadi, say]  \n",
       "4  [nah, dont, think, goe, usf, live, around, tho...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def stemming(tokenized_text):\n",
    "#     text = [ps.stem(word) for word in tokenized_text]\n",
    "#     return text\n",
    "\n",
    "# df['content_stemmed'] = df['content_nostop'].apply(lambda x: stemming(x))\n",
    "\n",
    "df['content_stemmed'] = df['content_nostop'].apply(lambda tokens: [ps.stem(i) for i in tokens])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ae3365",
   "metadata": {},
   "source": [
    "## Task6: Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcbc99f",
   "metadata": {},
   "source": [
    "###### Definition: Lemmatization reduces words to their lemma, which is the base or dictionary form of a word, ensuring the result is a valid word in the language.\n",
    "###### Approach: It considers the meaning and context of the word, requiring the word's part of speech (POS). For example, \"better\" is reduced to \"good\" (adjective), but \"am\" becomes \"be\" (verb).\n",
    "###### Output: The result is a valid word that makes sense in the language.\n",
    "###### Speed: Slower compared to stemming since it involves more complex linguistic processing.\n",
    "###### Accuracy: Higher accuracy due to understanding the word's context and meaning.\n",
    "###### Example:\n",
    "###### \"Caring\" → \"care\"\n",
    "###### \"Studies\" → \"study\"\n",
    "###### Common tools: WordNet Lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf4708e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20695fce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "      <th>content_clean</th>\n",
       "      <th>content_tokenized</th>\n",
       "      <th>content_nostop</th>\n",
       "      <th>content_stemmed</th>\n",
       "      <th>content_lemmetized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go dancing until jurong point, crazy.. Availab...</td>\n",
       "      <td>Go dancing until jurong point crazy Available ...</td>\n",
       "      <td>[go, dancing, until, jurong, point, crazy, ava...</td>\n",
       "      <td>[go, dancing, jurong, point, crazy, available,...</td>\n",
       "      <td>[go, danc, jurong, point, crazi, avail, bugi, ...</td>\n",
       "      <td>[go, dancing, jurong, point, crazy, available,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok better lar... Joking wif u oni...</td>\n",
       "      <td>Ok better lar Joking wif u oni</td>\n",
       "      <td>[ok, better, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, better, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, better, lar, joke, wif, u, oni]</td>\n",
       "      <td>[ok, better, lar, joking, wif, u, oni]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, fin...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "      <td>[u, dun, say, earli, hor, u, c, alreadi, say]</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, t...</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, tho...</td>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            content  \\\n",
       "0   ham  Go dancing until jurong point, crazy.. Availab...   \n",
       "1   ham               Ok better lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       content_clean  \\\n",
       "0  Go dancing until jurong point crazy Available ...   \n",
       "1                     Ok better lar Joking wif u oni   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3        U dun say so early hor U c already then say   \n",
       "4  Nah I dont think he goes to usf he lives aroun...   \n",
       "\n",
       "                                   content_tokenized  \\\n",
       "0  [go, dancing, until, jurong, point, crazy, ava...   \n",
       "1             [ok, better, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3  [u, dun, say, so, early, hor, u, c, already, t...   \n",
       "4  [nah, i, dont, think, he, goes, to, usf, he, l...   \n",
       "\n",
       "                                      content_nostop  \\\n",
       "0  [go, dancing, jurong, point, crazy, available,...   \n",
       "1             [ok, better, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "3      [u, dun, say, early, hor, u, c, already, say]   \n",
       "4  [nah, dont, think, goes, usf, lives, around, t...   \n",
       "\n",
       "                                     content_stemmed  \\\n",
       "0  [go, danc, jurong, point, crazi, avail, bugi, ...   \n",
       "1               [ok, better, lar, joke, wif, u, oni]   \n",
       "2  [free, entri, 2, wkli, comp, win, fa, cup, fin...   \n",
       "3      [u, dun, say, earli, hor, u, c, alreadi, say]   \n",
       "4  [nah, dont, think, goe, usf, live, around, tho...   \n",
       "\n",
       "                                  content_lemmetized  \n",
       "0  [go, dancing, jurong, point, crazy, available,...  \n",
       "1             [ok, better, lar, joking, wif, u, oni]  \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, fin...  \n",
       "3      [u, dun, say, early, hor, u, c, already, say]  \n",
       "4  [nah, dont, think, go, usf, life, around, though]  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def lemmatizing(tokenized_text):\n",
    "#     text = [wn.lemmatize(word) for word in tokenized_text]\n",
    "#     return text\n",
    "\n",
    "# df['content_lemmetized'] = df['content_nostop'].apply(lambda x: lemmatizing(x))\n",
    "\n",
    "df['content_lemmetized'] = df['content_nostop'].apply(lambda x: [wn.lemmatize(i) for i in x])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cdd48e",
   "metadata": {},
   "source": [
    "## Task7: Vectorizing (Count Vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764a9a9c",
   "metadata": {},
   "source": [
    "###### Vectorizing in Natural Language Processing (NLP) is the process of converting text data into numerical format so that machine learning models can understand and process it. Text data, in its raw form, consists of strings, which models cannot work with. Vectorization transforms this text into vectors, i.e., arrays of numbers, making it usable for algorithms.\n",
    "\n",
    "##### Why is Vectorizing Important?\n",
    "###### Machine learning algorithms need numbers: Algorithms can only process numerical data. Since text is inherently non-numeric, vectorizing converts text into a numeric format.\n",
    "###### Representation of text: Vectorization encodes information about the structure, meaning, and context of the text into vectors that models can use for predictions and analysis.\n",
    "###### Feature extraction: It allows the model to capture key features from the text such as word frequency, relationships between words, or semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81234294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f49e6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_content'] = df['content_lemmetized'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d90b183",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "      <th>content_clean</th>\n",
       "      <th>content_tokenized</th>\n",
       "      <th>content_nostop</th>\n",
       "      <th>content_stemmed</th>\n",
       "      <th>content_lemmetized</th>\n",
       "      <th>cleaned_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go dancing until jurong point, crazy.. Availab...</td>\n",
       "      <td>Go dancing until jurong point crazy Available ...</td>\n",
       "      <td>[go, dancing, until, jurong, point, crazy, ava...</td>\n",
       "      <td>[go, dancing, jurong, point, crazy, available,...</td>\n",
       "      <td>[go, danc, jurong, point, crazi, avail, bugi, ...</td>\n",
       "      <td>[go, dancing, jurong, point, crazy, available,...</td>\n",
       "      <td>go dancing jurong point crazy available bugis ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok better lar... Joking wif u oni...</td>\n",
       "      <td>Ok better lar Joking wif u oni</td>\n",
       "      <td>[ok, better, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, better, lar, joking, wif, u, oni]</td>\n",
       "      <td>[ok, better, lar, joke, wif, u, oni]</td>\n",
       "      <td>[ok, better, lar, joking, wif, u, oni]</td>\n",
       "      <td>ok better lar joking wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>[free, entry, in, 2, a, wkly, comp, to, win, f...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>[free, entri, 2, wkli, comp, win, fa, cup, fin...</td>\n",
       "      <td>[free, entry, 2, wkly, comp, win, fa, cup, fin...</td>\n",
       "      <td>free entry 2 wkly comp win fa cup final tkts 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>U dun say so early hor U c already then say</td>\n",
       "      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "      <td>[u, dun, say, earli, hor, u, c, alreadi, say]</td>\n",
       "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
       "      <td>u dun say early hor u c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>Nah I dont think he goes to usf he lives aroun...</td>\n",
       "      <td>[nah, i, dont, think, he, goes, to, usf, he, l...</td>\n",
       "      <td>[nah, dont, think, goes, usf, lives, around, t...</td>\n",
       "      <td>[nah, dont, think, goe, usf, live, around, tho...</td>\n",
       "      <td>[nah, dont, think, go, usf, life, around, though]</td>\n",
       "      <td>nah dont think go usf life around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            content  \\\n",
       "0   ham  Go dancing until jurong point, crazy.. Availab...   \n",
       "1   ham               Ok better lar... Joking wif u oni...   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3   ham  U dun say so early hor... U c already then say...   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "\n",
       "                                       content_clean  \\\n",
       "0  Go dancing until jurong point crazy Available ...   \n",
       "1                     Ok better lar Joking wif u oni   \n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3        U dun say so early hor U c already then say   \n",
       "4  Nah I dont think he goes to usf he lives aroun...   \n",
       "\n",
       "                                   content_tokenized  \\\n",
       "0  [go, dancing, until, jurong, point, crazy, ava...   \n",
       "1             [ok, better, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, in, 2, a, wkly, comp, to, win, f...   \n",
       "3  [u, dun, say, so, early, hor, u, c, already, t...   \n",
       "4  [nah, i, dont, think, he, goes, to, usf, he, l...   \n",
       "\n",
       "                                      content_nostop  \\\n",
       "0  [go, dancing, jurong, point, crazy, available,...   \n",
       "1             [ok, better, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "3      [u, dun, say, early, hor, u, c, already, say]   \n",
       "4  [nah, dont, think, goes, usf, lives, around, t...   \n",
       "\n",
       "                                     content_stemmed  \\\n",
       "0  [go, danc, jurong, point, crazi, avail, bugi, ...   \n",
       "1               [ok, better, lar, joke, wif, u, oni]   \n",
       "2  [free, entri, 2, wkli, comp, win, fa, cup, fin...   \n",
       "3      [u, dun, say, earli, hor, u, c, alreadi, say]   \n",
       "4  [nah, dont, think, goe, usf, live, around, tho...   \n",
       "\n",
       "                                  content_lemmetized  \\\n",
       "0  [go, dancing, jurong, point, crazy, available,...   \n",
       "1             [ok, better, lar, joking, wif, u, oni]   \n",
       "2  [free, entry, 2, wkly, comp, win, fa, cup, fin...   \n",
       "3      [u, dun, say, early, hor, u, c, already, say]   \n",
       "4  [nah, dont, think, go, usf, life, around, though]   \n",
       "\n",
       "                                     cleaned_content  \n",
       "0  go dancing jurong point crazy available bugis ...  \n",
       "1                     ok better lar joking wif u oni  \n",
       "2  free entry 2 wkly comp win fa cup final tkts 2...  \n",
       "3                u dun say early hor u c already say  \n",
       "4           nah dont think go usf life around though  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7c14e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 230)\n",
      "['09061701461 claim' '100 20000' '100000 prize' '11 month' '12 hour'\n",
      " '120 poboxox36504w45wq' '150 rcv' '150pday 6days' '16 tsandcs'\n",
      " '20000 pound' '2005 text' '21st may' '4txt 120' '6days 16' '81010 tc'\n",
      " '87077 eg' '87077 trywales' '87121 receive' '87575 cost' '900 prize'\n",
      " 'aid patent' 'already say' 'amore wat' 'anymore tonight'\n",
      " 'apply 08452810075over18s' 'apply reply' 'around though'\n",
      " 'available bugis' 'back id' 'better lar' 'blessing time'\n",
      " 'breather promise' 'brother like' 'buffet cine' 'bugis great'\n",
      " 'call 09061701461' 'call mobile' 'caller press' 'callertune caller'\n",
      " 'camera free' 'cash 100' 'chance win' 'chgs send' 'cine got'\n",
      " 'claim 81010' 'claim call' 'claim code' 'click httpwap' 'click wap'\n",
      " 'co free' 'code kl341' 'colour mobile' 'comp win' 'copy friend'\n",
      " 'cost 150pday' 'crazy available' 'credit click' 'cried enough'\n",
      " 'csh11 send' 'cup final' 'customer selected' 'dancing jurong'\n",
      " 'darling week' 'date sunday' 'dont miss' 'dont think' 'dont want'\n",
      " 'dun say' 'early hor' 'eg england' 'eh remember' 'england 87077'\n",
      " 'england macedonia' 'enough today' 'entitled update' 'entry questionstd'\n",
      " 'entry wkly' 'even brother' 'fa 87121' 'fa cup' 'feel that' 'final tkts'\n",
      " 'fine that' 'free 08002986030' 'free call' 'free entry' 'free membership'\n",
      " 'freemsg hey' 'friend callertune' 'fulfil promise' 'fun still'\n",
      " 'go dancing' 'go usf' 'goalsteam news' 'gon na' 'got amore'\n",
      " 'granted fulfil' 'great world' 'help granted' 'hey darling' 'hl info'\n",
      " 'home soon' 'hor already' 'httpwap xxxmobilemovieclubcomnqjkgighjjgcbl'\n",
      " 'id like' 'im gon' 'ive cried' 'ive searching' 'jackpot txt' 'joking wif'\n",
      " 'jurong point' 'kim watching' 'kl341 valid' 'la buffet' 'lar joking'\n",
      " 'latest colour' 'lccltd pobox' 'life around' 'like aid' 'like fun'\n",
      " 'like speak' 'link next' 'macedonia dont' 'make wet' 'may 2005'\n",
      " 'melle melle' 'melle oru' 'membership 100000' 'message click'\n",
      " 'minnaminunginte nurungu' 'miss goalsteam' 'mobile 11' 'mobile camera'\n",
      " 'mobile update' 'month entitled' 'na home' 'nah dont' 'name yes'\n",
      " 'national team' 'naughty make' 'network customer' 'news txt' 'next txt'\n",
      " 'nurungu vettam' 'oh kim' 'ok better' 'ok xxx' 'oru minnaminunginte'\n",
      " 'per request' 'pobox 4403ldnw1a7rw18' 'poboxox36504w45wq 16'\n",
      " 'point crazy' 'pound txt' 'press copy' 'prize jackpot' 'prize reward'\n",
      " 'promise wonderful' 'promise wont' 'questionstd txt' 'ratetcs apply'\n",
      " 'receive entry' 'receivea 900' 'remember spell' 'reply hl'\n",
      " 'request melle' 'reward claim' 'right word' 'say early' 'scotland 4txt'\n",
      " 'searching right' 'selected receivea' 'send 150' 'send 87575'\n",
      " 'set callertune' 'six chance' 'soon dont' 'speak treat' 'spell name'\n",
      " 'std chgs' 'still tb' 'stuff anymore' 'take help' 'talk stuff' 'tb ok'\n",
      " 'tc wwwdbuknet' 'team 87077' 'text fa' 'thank breather' 'that way'\n",
      " 'think go' 'tkts 21st' 'tonight ive' 'treat like' 'trywales scotland'\n",
      " 'tsandcs apply' 'txt csh11' 'txt message' 'txt ratetcs' 'txt ur'\n",
      " 'txt word' 'update co' 'update latest' 'ur national' 'urgent week'\n",
      " 'use credit' 'usf life' 'valid 12' 'valued network' 'vettam set'\n",
      " 'want talk' 'wap link' 'way feel' 'way gota' 'week free' 'week word'\n",
      " 'wif oni' 'win cash' 'win fa' 'winner valued' 'wkly comp'\n",
      " 'wonderful blessing' 'wont take' 'word back' 'word claim' 'word thank'\n",
      " 'world la' 'wwwdbuknet lccltd' 'xxx std' 'xxxmobilemovieclub use'\n",
      " 'yes naughty']\n"
     ]
    }
   ],
   "source": [
    "# this task is for sample displaying\n",
    "data_sample = df[0:20]\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(2,2))\n",
    "result_vector = cv.fit_transform(data_sample['cleaned_content'])\n",
    "print(result_vector.shape)\n",
    "print(cv.get_feature_names_out())\n",
    "feature_names = list(cv.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac6cc5db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['09061701461 claim',\n",
       " '100 20000',\n",
       " '100000 prize',\n",
       " '11 month',\n",
       " '12 hour',\n",
       " '120 poboxox36504w45wq',\n",
       " '150 rcv',\n",
       " '150pday 6days',\n",
       " '16 tsandcs',\n",
       " '20000 pound',\n",
       " '2005 text',\n",
       " '21st may',\n",
       " '4txt 120',\n",
       " '6days 16',\n",
       " '81010 tc',\n",
       " '87077 eg',\n",
       " '87077 trywales',\n",
       " '87121 receive',\n",
       " '87575 cost',\n",
       " '900 prize',\n",
       " 'aid patent',\n",
       " 'already say',\n",
       " 'amore wat',\n",
       " 'anymore tonight',\n",
       " 'apply 08452810075over18s',\n",
       " 'apply reply',\n",
       " 'around though',\n",
       " 'available bugis',\n",
       " 'back id',\n",
       " 'better lar',\n",
       " 'blessing time',\n",
       " 'breather promise',\n",
       " 'brother like',\n",
       " 'buffet cine',\n",
       " 'bugis great',\n",
       " 'call 09061701461',\n",
       " 'call mobile',\n",
       " 'caller press',\n",
       " 'callertune caller',\n",
       " 'camera free',\n",
       " 'cash 100',\n",
       " 'chance win',\n",
       " 'chgs send',\n",
       " 'cine got',\n",
       " 'claim 81010',\n",
       " 'claim call',\n",
       " 'claim code',\n",
       " 'click httpwap',\n",
       " 'click wap',\n",
       " 'co free',\n",
       " 'code kl341',\n",
       " 'colour mobile',\n",
       " 'comp win',\n",
       " 'copy friend',\n",
       " 'cost 150pday',\n",
       " 'crazy available',\n",
       " 'credit click',\n",
       " 'cried enough',\n",
       " 'csh11 send',\n",
       " 'cup final',\n",
       " 'customer selected',\n",
       " 'dancing jurong',\n",
       " 'darling week',\n",
       " 'date sunday',\n",
       " 'dont miss',\n",
       " 'dont think',\n",
       " 'dont want',\n",
       " 'dun say',\n",
       " 'early hor',\n",
       " 'eg england',\n",
       " 'eh remember',\n",
       " 'england 87077',\n",
       " 'england macedonia',\n",
       " 'enough today',\n",
       " 'entitled update',\n",
       " 'entry questionstd',\n",
       " 'entry wkly',\n",
       " 'even brother',\n",
       " 'fa 87121',\n",
       " 'fa cup',\n",
       " 'feel that',\n",
       " 'final tkts',\n",
       " 'fine that',\n",
       " 'free 08002986030',\n",
       " 'free call',\n",
       " 'free entry',\n",
       " 'free membership',\n",
       " 'freemsg hey',\n",
       " 'friend callertune',\n",
       " 'fulfil promise',\n",
       " 'fun still',\n",
       " 'go dancing',\n",
       " 'go usf',\n",
       " 'goalsteam news',\n",
       " 'gon na',\n",
       " 'got amore',\n",
       " 'granted fulfil',\n",
       " 'great world',\n",
       " 'help granted',\n",
       " 'hey darling',\n",
       " 'hl info',\n",
       " 'home soon',\n",
       " 'hor already',\n",
       " 'httpwap xxxmobilemovieclubcomnqjkgighjjgcbl',\n",
       " 'id like',\n",
       " 'im gon',\n",
       " 'ive cried',\n",
       " 'ive searching',\n",
       " 'jackpot txt',\n",
       " 'joking wif',\n",
       " 'jurong point',\n",
       " 'kim watching',\n",
       " 'kl341 valid',\n",
       " 'la buffet',\n",
       " 'lar joking',\n",
       " 'latest colour',\n",
       " 'lccltd pobox',\n",
       " 'life around',\n",
       " 'like aid',\n",
       " 'like fun',\n",
       " 'like speak',\n",
       " 'link next',\n",
       " 'macedonia dont',\n",
       " 'make wet',\n",
       " 'may 2005',\n",
       " 'melle melle',\n",
       " 'melle oru',\n",
       " 'membership 100000',\n",
       " 'message click',\n",
       " 'minnaminunginte nurungu',\n",
       " 'miss goalsteam',\n",
       " 'mobile 11',\n",
       " 'mobile camera',\n",
       " 'mobile update',\n",
       " 'month entitled',\n",
       " 'na home',\n",
       " 'nah dont',\n",
       " 'name yes',\n",
       " 'national team',\n",
       " 'naughty make',\n",
       " 'network customer',\n",
       " 'news txt',\n",
       " 'next txt',\n",
       " 'nurungu vettam',\n",
       " 'oh kim',\n",
       " 'ok better',\n",
       " 'ok xxx',\n",
       " 'oru minnaminunginte',\n",
       " 'per request',\n",
       " 'pobox 4403ldnw1a7rw18',\n",
       " 'poboxox36504w45wq 16',\n",
       " 'point crazy',\n",
       " 'pound txt',\n",
       " 'press copy',\n",
       " 'prize jackpot',\n",
       " 'prize reward',\n",
       " 'promise wonderful',\n",
       " 'promise wont',\n",
       " 'questionstd txt',\n",
       " 'ratetcs apply',\n",
       " 'receive entry',\n",
       " 'receivea 900',\n",
       " 'remember spell',\n",
       " 'reply hl',\n",
       " 'request melle',\n",
       " 'reward claim',\n",
       " 'right word',\n",
       " 'say early',\n",
       " 'scotland 4txt',\n",
       " 'searching right',\n",
       " 'selected receivea',\n",
       " 'send 150',\n",
       " 'send 87575',\n",
       " 'set callertune',\n",
       " 'six chance',\n",
       " 'soon dont',\n",
       " 'speak treat',\n",
       " 'spell name',\n",
       " 'std chgs',\n",
       " 'still tb',\n",
       " 'stuff anymore',\n",
       " 'take help',\n",
       " 'talk stuff',\n",
       " 'tb ok',\n",
       " 'tc wwwdbuknet',\n",
       " 'team 87077',\n",
       " 'text fa',\n",
       " 'thank breather',\n",
       " 'that way',\n",
       " 'think go',\n",
       " 'tkts 21st',\n",
       " 'tonight ive',\n",
       " 'treat like',\n",
       " 'trywales scotland',\n",
       " 'tsandcs apply',\n",
       " 'txt csh11',\n",
       " 'txt message',\n",
       " 'txt ratetcs',\n",
       " 'txt ur',\n",
       " 'txt word',\n",
       " 'update co',\n",
       " 'update latest',\n",
       " 'ur national',\n",
       " 'urgent week',\n",
       " 'use credit',\n",
       " 'usf life',\n",
       " 'valid 12',\n",
       " 'valued network',\n",
       " 'vettam set',\n",
       " 'want talk',\n",
       " 'wap link',\n",
       " 'way feel',\n",
       " 'way gota',\n",
       " 'week free',\n",
       " 'week word',\n",
       " 'wif oni',\n",
       " 'win cash',\n",
       " 'win fa',\n",
       " 'winner valued',\n",
       " 'wkly comp',\n",
       " 'wonderful blessing',\n",
       " 'wont take',\n",
       " 'word back',\n",
       " 'word claim',\n",
       " 'word thank',\n",
       " 'world la',\n",
       " 'wwwdbuknet lccltd',\n",
       " 'xxx std',\n",
       " 'xxxmobilemovieclub use',\n",
       " 'yes naughty']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbcdfb0",
   "metadata": {},
   "source": [
    "## Task8: Creating Vector DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25c3ac69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>09061701461 claim</th>\n",
       "      <th>100 20000</th>\n",
       "      <th>100000 prize</th>\n",
       "      <th>11 month</th>\n",
       "      <th>12 hour</th>\n",
       "      <th>120 poboxox36504w45wq</th>\n",
       "      <th>150 rcv</th>\n",
       "      <th>150pday 6days</th>\n",
       "      <th>16 tsandcs</th>\n",
       "      <th>20000 pound</th>\n",
       "      <th>...</th>\n",
       "      <th>wonderful blessing</th>\n",
       "      <th>wont take</th>\n",
       "      <th>word back</th>\n",
       "      <th>word claim</th>\n",
       "      <th>word thank</th>\n",
       "      <th>world la</th>\n",
       "      <th>wwwdbuknet lccltd</th>\n",
       "      <th>xxx std</th>\n",
       "      <th>xxxmobilemovieclub use</th>\n",
       "      <th>yes naughty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 230 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    09061701461 claim  100 20000  100000 prize  11 month  12 hour  \\\n",
       "0                   0          0             0         0        0   \n",
       "1                   0          0             0         0        0   \n",
       "2                   0          0             0         0        0   \n",
       "3                   0          0             0         0        0   \n",
       "4                   0          0             0         0        0   \n",
       "5                   0          0             0         0        0   \n",
       "6                   0          0             0         0        0   \n",
       "7                   0          0             0         0        0   \n",
       "8                   1          0             0         0        1   \n",
       "9                   0          0             0         1        0   \n",
       "10                  0          0             0         0        0   \n",
       "11                  0          1             0         0        0   \n",
       "12                  0          0             1         0        0   \n",
       "13                  0          0             0         0        0   \n",
       "14                  0          0             0         0        0   \n",
       "15                  0          0             0         0        0   \n",
       "16                  0          0             0         0        0   \n",
       "17                  0          0             0         0        0   \n",
       "18                  0          0             0         0        0   \n",
       "19                  0          0             0         0        0   \n",
       "\n",
       "    120 poboxox36504w45wq  150 rcv  150pday 6days  16 tsandcs  20000 pound  \\\n",
       "0                       0        0              0           0            0   \n",
       "1                       0        0              0           0            0   \n",
       "2                       0        0              0           0            0   \n",
       "3                       0        0              0           0            0   \n",
       "4                       0        0              0           0            0   \n",
       "5                       0        1              0           0            0   \n",
       "6                       0        0              0           0            0   \n",
       "7                       0        0              0           0            0   \n",
       "8                       0        0              0           0            0   \n",
       "9                       0        0              0           0            0   \n",
       "10                      0        0              0           0            0   \n",
       "11                      0        0              1           1            1   \n",
       "12                      0        0              0           0            0   \n",
       "13                      0        0              0           0            0   \n",
       "14                      0        0              0           0            0   \n",
       "15                      0        0              0           0            0   \n",
       "16                      0        0              0           0            0   \n",
       "17                      0        0              0           0            0   \n",
       "18                      0        0              0           0            0   \n",
       "19                      1        0              0           0            0   \n",
       "\n",
       "    ...  wonderful blessing  wont take  word back  word claim  word thank  \\\n",
       "0   ...                   0          0          0           0           0   \n",
       "1   ...                   0          0          0           0           0   \n",
       "2   ...                   0          0          0           0           0   \n",
       "3   ...                   0          0          0           0           0   \n",
       "4   ...                   0          0          0           0           0   \n",
       "5   ...                   0          0          1           0           0   \n",
       "6   ...                   0          0          0           0           0   \n",
       "7   ...                   0          0          0           0           0   \n",
       "8   ...                   0          0          0           0           0   \n",
       "9   ...                   0          0          0           0           0   \n",
       "10  ...                   0          0          0           0           0   \n",
       "11  ...                   0          0          0           0           0   \n",
       "12  ...                   0          0          0           1           0   \n",
       "13  ...                   1          1          0           0           1   \n",
       "14  ...                   0          0          0           0           0   \n",
       "15  ...                   0          0          0           0           0   \n",
       "16  ...                   0          0          0           0           0   \n",
       "17  ...                   0          0          0           0           0   \n",
       "18  ...                   0          0          0           0           0   \n",
       "19  ...                   0          0          0           0           0   \n",
       "\n",
       "    world la  wwwdbuknet lccltd  xxx std  xxxmobilemovieclub use  yes naughty  \n",
       "0          1                  0        0                       0            0  \n",
       "1          0                  0        0                       0            0  \n",
       "2          0                  0        0                       0            0  \n",
       "3          0                  0        0                       0            0  \n",
       "4          0                  0        0                       0            0  \n",
       "5          0                  0        1                       0            0  \n",
       "6          0                  0        0                       0            0  \n",
       "7          0                  0        0                       0            0  \n",
       "8          0                  0        0                       0            0  \n",
       "9          0                  0        0                       0            0  \n",
       "10         0                  0        0                       0            0  \n",
       "11         0                  0        0                       0            0  \n",
       "12         0                  1        0                       0            0  \n",
       "13         0                  0        0                       0            0  \n",
       "14         0                  0        0                       0            0  \n",
       "15         0                  0        0                       1            0  \n",
       "16         0                  0        0                       0            0  \n",
       "17         0                  0        0                       0            1  \n",
       "18         0                  0        0                       0            0  \n",
       "19         0                  0        0                       0            0  \n",
       "\n",
       "[20 rows x 230 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_vector_df = pd.DataFrame(result_vector.toarray())\n",
    "result_vector_df.columns = cv.get_feature_names_out()\n",
    "result_vector_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce01311",
   "metadata": {},
   "source": [
    "## Task9: Text Classification Model Building (Tfidf Vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94d8f2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 4800)\t0.20086730050076274\n",
      "  (0, 683)\t0.3522921777903761\n",
      "  (0, 2081)\t0.16364206949326077\n",
      "  (0, 1371)\t0.29761785540555796\n",
      "  (0, 1221)\t0.33630096361664713\n",
      "  (0, 2481)\t0.29761785540555796\n",
      "  (0, 4905)\t0.23940930234707425\n",
      "  (0, 2095)\t0.19608092698093615\n",
      "  (0, 1222)\t0.29761785540555796\n",
      "  (0, 906)\t0.2678897084235422\n",
      "  (0, 1495)\t0.27282604100952795\n",
      "  (0, 3133)\t0.24055254723113312\n",
      "  (0, 1538)\t0.32495501659796705\n",
      "  (0, 2059)\t0.15656162093995568\n",
      "  (1, 2987)\t0.5072509658023376\n",
      "  (1, 4859)\t0.4005392983095089\n",
      "  (1, 2406)\t0.485959732574947\n",
      "  (1, 2494)\t0.3789141443562463\n",
      "  (1, 1094)\t0.3710921213772139\n",
      "  (1, 2975)\t0.2546027792003474\n",
      "  (2, 34)\t0.23315708783038774\n",
      "  (2, 767)\t0.16843248397857818\n",
      "  (2, 3315)\t0.23315708783038774\n",
      "  (2, 4645)\t0.12529349927345543\n",
      "  (2, 3264)\t0.23315708783038774\n",
      "  :\t:\n",
      "  (5567, 1375)\t0.22551721284727191\n",
      "  (5567, 3200)\t0.23625789118119672\n",
      "  (5567, 3069)\t0.2548234732351594\n",
      "  (5568, 1788)\t0.6457639024659789\n",
      "  (5568, 1958)\t0.5628210076825846\n",
      "  (5568, 2065)\t0.3636095248537397\n",
      "  (5568, 2216)\t0.3660732290385455\n",
      "  (5569, 4083)\t0.6198322100581006\n",
      "  (5569, 4385)\t0.5916968433151221\n",
      "  (5569, 2809)\t0.5154637494387073\n",
      "  (5570, 511)\t0.39582830697630306\n",
      "  (5570, 2333)\t0.35029805018753357\n",
      "  (5570, 2115)\t0.255243808384826\n",
      "  (5570, 1236)\t0.3448305421148193\n",
      "  (5570, 2019)\t0.3355153751106562\n",
      "  (5570, 1755)\t0.2974053215620411\n",
      "  (5570, 4107)\t0.2509558235260118\n",
      "  (5570, 2900)\t0.2559986244637082\n",
      "  (5570, 2544)\t0.19438952733965167\n",
      "  (5570, 2275)\t0.2845176102491725\n",
      "  (5570, 4822)\t0.22382261724695712\n",
      "  (5570, 1962)\t0.19540979183907345\n",
      "  (5571, 3574)\t0.6901221305604214\n",
      "  (5571, 4620)\t0.5366272304828582\n",
      "  (5571, 2860)\t0.48555397271059575\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "#Prepare data and labels\n",
    "X = df['cleaned_content']\n",
    "y = df['label']\n",
    "\n",
    "#TfId Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
    "print(X_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4050e539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(random_state=42)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "#Split data into train and test sets\n",
    "X_train, X_test,y_train,y_test = train_test_split(X_tfidf, y,test_size=0.2,random_state=42)\n",
    "\n",
    "#Initialize SVM Classifier\n",
    "svm = LinearSVC(random_state=42)\n",
    "\n",
    "#train the classifier\n",
    "svm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2335f571",
   "metadata": {},
   "source": [
    "## Task10: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "75b3d2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.979372197309417\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99       965\n",
      "        spam       0.98      0.86      0.92       150\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.98      0.93      0.95      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#predictions\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_test,y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "#classification report\n",
    "report = classification_report(y_test,y_pred)\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
